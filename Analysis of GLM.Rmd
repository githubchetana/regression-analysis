---
title: "Analysis of Genral Linear Model"
author: "Chetana Jamadar"
date: "2022-12-16"
output: html_document
---
1. Consider the following model
a) y1 = β1 +β2 +ε1, y2 = β1 +β3 +ε2, y3 = β1 +β2 +ε3.
b) yi = β1 +β2xi +εi
, (i = 1, 2, 3) where x1 = −1, x2 = 0, x3 = 1.
Write these models in Gauss-Markoff setup. Find the blues of βi s.

```{r}
x=matrix(c(1,1,0,1,0,1,1,1,0),nrow=3,ncol=3,byrow=T)
x
library(Matrix)
r=rankMatrix(x)#Rank matrix
r[1]
n=3
n-r;
# S,G,H
S=t(x)%*%x;
S
r=rankMatrix(S)
r
#p=3 and r=2 i.e r<p
#so the given model is non full rank model
library(MASS)
G=ginv(S)
G
H=G%*%S
H
#To Check Estimability of β1-β2
l1=matrix(c(0,1,-1),3,1)
t(l1)
zapsmall(t(l1)%*%H)
```

2. Consider the following model.
yi = θi +θ5+εi
, i = 1, 2, 3y6 = θ3+θ7+ε6 y4 = θ4+θ6+ε4 y7 = θ2+θ8+ε7 y5 = θ1+θ7+ε5 y8 = θ4+θ8+ε8
(a) Write the model in Gauss-Markoff setup and find rank of error space and estimation space.
(b) Check the estimability of the following parametric functions.
i) θ2 −θ4 ii) θ1 +θ2 +θ3 +3θ5 iii) θ1 −θ2 iv) θ3 +2θ5 −θ1 −2θ8
(c) If Y = (60.2 74.39 77.88 94.75 81.47 99.34 111.86 127.68)
obtain two different solutions to normal equations and verify that the BLUE of an estimable parametric function is unique even though two different solutions to normal equations are used.
(d) Obtain an estimate of error variance, BLUE of Xβ and variance-covariance of the BLUE of
Xβ.
```{r}
#a)
n=8;
p=8;
x1=c(1,0,0,0,1,0,0,0)
x2=c(0,1,0,0,1,0,0,0)
x3=c(0,0,1,0,1,0,0,0)
x4=c(0,0,0,1,0,1,0,0)
x5=c(0,0,0,1,0,0,1,0)
x6=c(0,0,1,0,0,0,1,0)
x7=c(0,1,0,0,0,0,0,1)
x8=c(0,0,0,1,0,0,0,1)
X=matrix(c(x1,x2,x3,x4,x7,x8),nrow = 8,ncol = 8,byrow = T);
X=rbind(x1,x2,x3,x4,x5,x7,x8);
X
library(Matrix)
r=rankMatrix(X)
r[1]
#Here R<p the given model is non full rank model
n-r#Rank of error space
#To find S,G,H
S=t(X)%*%X
r=rankMatrix(S)
library(MASS)
G=ginv(S)
H=G%*%S
#B)
#1)To check estimability of θ1-θ4.
l1=Matrix(c(0,1,0,-1,0,0,0,0),8,1)
l1
t(l1)
zapsmall(t(l1)%*%H)
#2) To check the estimability θ1 +θ2 +θ3 +3θ5
l2=matrix(c(1,1,1,0,3,0,0,0),8,1,1);
l2
t(l2)
zapsmall(t(l2)%*%H)
#3)To check estimability of θ1 −θ2
l3=matrix(c(1,-1,0,0,0,0,0,0),8,1,1)
t(l3);
zapsmall(t(l3)%*%H)
#)4) θ3 +2θ5 −θ1 −2θ8
l4=matrix(c(-1,0,1,0,2,0,0,-2),8,1,1)
t(l4);
zapsmall(t(l4)%*%H)
#c)To obtain Normal parametric function
Y=matrix(c(60.2,74.39,77.88,94.75,81.47,99.34,111.86,127.68));
beta_hat=G%*%t(X)%*%Y
Z=matrix(c(0,1,0,0.5,0,1,0,4),8,1)
Z
beta_tilda=beta_hat+(diag(8)-H)%*%Z
t(l1)%*%beta_hat
t(l1)%*%beta_tilda
t(l2)%*%beta_hat
t(l2)%*%beta_tilda
t(l3)%*%beta_hat
t(l3)%*%beta_tilda
t(l4)%*%beta_hat
t(l4)%*%beta_tilda
#d)To estimate error variance
Y_hat=X%*%beta_hat
cbind(Y,Y_hat)
e=Y-Y_hat
SSE=t(e)%*%e
sigma2_hat=SSE/(n-r)
est_var_1=t(l1)%*%t(G)%*%l1*Sigma2_hat;est_var_1
est_cov_1=t(l1)%*%G%*%l3*Sigma2_hat;est_cov_1
est_var_1=t(l2)%*%t(G)%*%l1*Sigma2_hat;est_var_1
est_cov_1=t(l2)%*%G%*%l3*Sigma2_hat;est_cov_1
est_var_1=t(l3)%*%t(G)%*%l1*Sigma2_hat;est_var_1
est_cov_1=t(l3)%*%G%*%l3*Sigma2_hat;est_cov_1
est_var_1=t(l4)%*%t(G)%*%l1*Sigma2_hat;est_var_1
est_cov_1=t(l4)%*%G%*%l3*Sigma2_hat;est_cov_1

```
3. Consider the following model.
y1j = θ1 +αj +ε1i, j = 1, 2, 4,y2j = θ2 +αj +ε2i, j = 2, 3, 4
y3j = θ3 +αj +ε3i, j = 1, 2, 3,y4j = θ4 +αj +ε4i, j = 1, 3, 4
(a) Write the model in Gauss-Markoff setup and find rank of error space and estimation space.
(b) Check the estimability of the following parametric functions.i) θ1 +α3 ii) θ1 +θ2 +θ3 −3θ4 iii) 3θ2 +α2 +α3 +α4 iv) θ1 −2θ4 +α3.
(c) Obtain solution of normal equations and hence obtain the BLUE and variance of BLUE of
any one estimable parametric functions in (b) using
y11, y12, y14, y22, y23, y24, y31, y32, y33, y41, y43, y44 =(73, 74, 71, 75, 67, 72, 73, 75, 68, 75, 72, 75)
```{r}
rm(list=ls(all=TRUE))
x1=c(1,0,0,0,1,0,0,0)
x2=c(1,0,0,0,0,1,0,0)
x3=c(1,0,0,0,0,0,0,1)
x4=c(0,1,0,0,0,1,0,0)
x5=c(0,1,0,0,0,0,1,0)
x6=c(0,1,0,0,0,0,0,1)
x7=c(0,0,1,0,1,0,0,0)
x8=c(0,0,1,0,0,1,0,0)
x9=c(0,0,1,0,0,0,1,0)
x10=c(0,0,0,1,1,0,0,0)
x11=c(0,0,0,1,0,0,1,0)
x12=c(0,0,0,1,0,0,0,1)
X=Matrix(c(x1,x2,x3,x4,x7,x8,x9,x10,x11,x12),nrow = 12,ncol = 12,byrow = T);
X=rbind(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12)
X
library(Matrix)
r=rankMatrix(X)
r=r[1]
r # rank of estimation space
n=12;
n-r # rank of error space
S=t(X)%*%X
library(MASS)
G=ginv(S)
H=G%*%S
# Check the estimability of theta1 + alpha3
l1=matrix(c(1,0,0,0,0,0,1,0),8,1,1)
t(l1)
zapsmall(t(l1)%*%H)
# Check the estimability of theta1+theta2+theta3-3*theta3
l2=matrix(c(1,1,1,-3,0,0,0,0),8,1,1)
t(l2)
zapsmall(t(l2)%*%H)
# to find solution to the normal equation
Y=matrix(c(73,74,71,75,67,72,73,75,68,75,72,75),12,1,1)
beta_hat=G%*%t(X)%*%Y
beta_hat
t(l1)%*%beta_hat # BLUE for e.l.p.f theta1+alpha3
# to estimate the sigma^2
e=Y-X%*%beta_hat;
sigma2_hat=t(e)%*%e/(n-r)
sigma2_hat
#to estimate variance of theta1_hat+alpha3_hat
var_1=t(l1)%*%t(G)%*%l1*sigma2_hat
var_1
```
